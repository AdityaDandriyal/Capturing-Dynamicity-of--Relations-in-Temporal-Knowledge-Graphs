{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocbIRbfrQa49",
        "outputId": "c94b99f8-f2bb-4cfd-fc58-aed70eaf3de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beverly_Adams\twasBornIn\tEdmonton\t1945-11-07\t1945-11-07\n",
            "Al_Gore\twasBornIn\tWashington,_D.C.\t1948-03-31\t1948-03-31\n",
            "Donald_R._McMonagle\twasBornIn\tFlint,_Michigan\t1952-05-14\t1952-05-14\n",
            "Donald_B._Beary\twasBornIn\tHelena,_Montana\t1888-12-04\t1888-12-04\n",
            "Ida_Halpern\twasBornIn\tVienna\t1910-07-17\t1910-07-17\n",
            "João_Paiva\twasBornIn\tLisbon\t1983-02-08\t1983-02-08\n",
            "Vincent_Wong_(Hong_Kong_actor)\twasBornIn\tHong_Kong\t1983-07-07\t1983-07-07\n",
            "Julie_Bishop_(actress)\twasBornIn\tDenver\t1914-08-30\t1914-08-30\n",
            "Bradley_Cooper\twasBornIn\tPhiladelphia\t1975-01-05\t1975-01-05\n",
            "José_Gonçalves_(footballer)\twasBornIn\tLisbon\t1985-09-17\t1985-09-17\n",
            "Harry_Weese\twasBornIn\tEvanston,_Illinois\t1915-06-30\t1915-06-30\n",
            "Roger_MacBride\twasBornIn\tNew_Rochelle,_New_York\t1929-08-06\t1929-08-06\n",
            "James_Neill\twasBornIn\tSavannah,_Georgia\t1860-09-29\t1860-09-29\n",
            "Wendy_Moniz\twasBornIn\tKansas_City,_Missouri\t1969-01-19\t1969-01-19\n",
            "Rob_Wagner\twasBornIn\tDetroit\t1872-08-02\t1872-08-02\n"
          ]
        }
      ],
      "source": [
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/hyte_text_data.txt'\n",
        "\n",
        "# Specify the number of lines to read\n",
        "num_lines_to_read = 15\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the specified number of lines\n",
        "    for _ in range(num_lines_to_read):\n",
        "        line = file.readline()\n",
        "\n",
        "        # Check if the line is not empty (end of file)\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "        # Process or print the line\n",
        "        print(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/TEMPORAL_filtered_literal_data.txt'\n",
        "\n",
        "# Specify the number of lines to read\n",
        "num_lines_to_read = 15\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the specified number of lines\n",
        "    for _ in range(num_lines_to_read):\n",
        "        line = file.readline()\n",
        "\n",
        "        # Check if the line is not empty (end of file)\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "        # Process or print the line\n",
        "        print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65ZobKNhRU8W",
        "outputId": "ebc83f9b-65bc-4723-bcd0-8add98ca1e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jens_Petersen\twasBornOnDate\t1941\t1941\t1941\n",
            "Avery_Brundage\tdiedOnDate\t1975\t1975\t1975\n",
            "Werner_Herzog\twasBornOnDate\t1942\t1942\t1942\n",
            "Ngaio_Marsh\twasCreatedOnDate\t2010\t2010\t2010\n",
            "Layne_Staley\twasDestroyedOnDate\t1987\t1987\t1987\n",
            "Anne_of_Austria,_Queen_of_Poland\tdiedOnDate\t1598\t1598\t1598\n",
            "Alessandro_Manzoni\tdiedOnDate\t1873\t1873\t1873\n",
            "Cornelius_Castoriadis\twasCreatedOnDate\t2000\t2000\t2000\n",
            "Bradley_Bell\twasBornOnDate\t1964\t1964\t1964\n",
            "Joseph_Jackson_(screenwriter)\twasBornOnDate\t1894\t1894\t1894\n",
            "Sidney_Luft\tdiedOnDate\t2005\t2005\t2005\n",
            "Dewi_Zephaniah_Phillips\tdiedOnDate\t2006\t2006\t2006\n",
            "Erskine_Hamilton_Childers\tdiedOnDate\t1974\t1974\t1974\n",
            "Luigi_Ferrero\twasBornOnDate\t1904\t1904\t1904\n",
            "John_Fante\tdiedOnDate\t1983\t1983\t1983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file path\n",
        "input_file_path = '/content/drive/MyDrive/hyte_text_data.txt'\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/drive/MyDrive/class_entity2id.txt'\n",
        "\n",
        "# Dictionary to store unique values and corresponding integers\n",
        "unique_values_dict = {}\n",
        "\n",
        "# Read the input file and extract unique values\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    for line in input_file:\n",
        "        # Split the line into elements\n",
        "        elements = line.strip().split()\n",
        "\n",
        "        # Check if there's at least one element\n",
        "        if elements:\n",
        "            # Extract the first element\n",
        "            first_element = elements[0]\n",
        "\n",
        "            # If the value is not in the dictionary, add it with a unique integer\n",
        "            if first_element not in unique_values_dict:\n",
        "                unique_values_dict[first_element] = len(unique_values_dict)\n",
        "\n",
        "# Read the input file and extract unique values\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    for line in input_file:\n",
        "        # Split the line into elements\n",
        "        elements = line.strip().split()\n",
        "\n",
        "        # Check if there's at least one element\n",
        "        if elements:\n",
        "            # Extract the first element\n",
        "            third_element = elements[2]\n",
        "\n",
        "            # If the value is not in the dictionary, add it with a unique integer\n",
        "            if third_element not in unique_values_dict:\n",
        "                unique_values_dict[third_element] = len(unique_values_dict)\n",
        "\n",
        "# Write unique values with <> symbols and unique integers to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for value, unique_integer in unique_values_dict.items():\n",
        "        output_file.write(f'<{value}>  \\t{unique_integer}\\n')"
      ],
      "metadata": {
        "id": "Xpc5N-GoRU_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " unique_values_dict"
      ],
      "metadata": {
        "id": "IsLGyDpgUiAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/drive/MyDrive/TEMPORAL_filtered_literal_data.txt'\n",
        "output_file_path = '/content/drive/MyDrive/class_entity2id.txt'\n",
        "\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    for line in input_file:\n",
        "        # Split the line into elements\n",
        "        elements = line.strip().split()\n",
        "\n",
        "        # Check if there's at least one element\n",
        "        if elements:\n",
        "            # Extract the first element\n",
        "            third_element = elements[0]\n",
        "\n",
        "            # If the value is not in the dictionary, add it with a unique integer\n",
        "            if third_element not in unique_values_dict:\n",
        "                unique_values_dict[third_element] = len(unique_values_dict)\n",
        "\n",
        "# Write unique values with <> symbols and unique integers to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for value, unique_integer in unique_values_dict.items():\n",
        "        output_file.write(f'<{value}>  \\t{unique_integer}\\n')"
      ],
      "metadata": {
        "id": "GDKBL3_YRVCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sorted_unique_values(input_file_path, column_index):\n",
        "    unique_values = set()\n",
        "\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        for line in input_file:\n",
        "            elements = line.strip().split('\\t')\n",
        "            if len(elements) > column_index:\n",
        "                unique_values.add(elements[column_index])\n",
        "\n",
        "    sorted_unique_values = sorted(unique_values)\n",
        "    return sorted_unique_values\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/TEMPORAL_filtered_literal_data.txt'  # Replace with your actual input file path\n",
        "\n",
        "# Get sorted unique values from the third column (column_index=2)\n",
        "sorted_unique_values_third_column = get_sorted_unique_values(input_file_path, 2)\n",
        "print(\"Sorted Unique Values in the Third Column:\")\n",
        "print(sorted_unique_values_third_column)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-bItXWDRVFE",
        "outputId": "3031ed46-d3f6-43a9-b5b6-7dfaf3fd8506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Unique Values in the Third Column:\n",
            "['0100', '0106', '0117', '0120', '0121', '0125', '0133', '0135', '0141', '0150', '0154', '0155', '0159', '0160', '0161', '0164', '0167', '0173', '0180', '0181', '0188', '0193', '0200', '0201', '0203', '0204', '0206', '0210', '0215', '0217', '0218', '0220', '0221', '0222', '0223', '0224', '0225', '0229', '0232', '0234', '0238', '0241', '0244', '0249', '0250', '0251', '0253', '0260', '0263', '0265', '0268', '0270', '0272', '0277', '0278', '0280', '0283', '0285', '0286', '0288', '0289', '0300', '0303', '0305', '0306', '0310', '0311', '0312', '0316', '0317', '0321', '0325', '0326', '0328', '0331', '0335', '0337', '0347', '0348', '0353', '0354', '0356', '0359', '0360', '0361', '0363', '0371', '0375', '0377', '0378', '0383', '0384', '0388', '0392', '0395', '0400', '0401', '0406', '0408', '0410', '0412', '0419', '0423', '0425', '0428', '0430', '0435', '0450', '0455', '0466', '0474', '0476', '0480', '0485', '0491', '0493', '0496', '0497', '0501', '0511', '0519', '0524', '0538', '0539', '0553', '0556', '0558', '0559', '0561', '0564', '0570', '0584', '0593', '0598', '0600', '0601', '0603', '0612', '0613', '0618', '0620', '0624', '0626', '0627', '0628', '0629', '0631', '0632', '0634', '0639', '0649', '0651', '0656', '0661', '0668', '0678', '0679', '0680', '0681', '0683', '0685', '0686', '0701', '0702', '0705', '0710', '0711', '0718', '0719', '0727', '0728', '0737', '0742', '0750', '0752', '0756', '0757', '0760', '0762', '0763', '0765', '0767', '0770', '0775', '0777', '0778', '0779', '0788', '0795', '0796', '0798', '0801', '0802', '0806', '0809', '0810', '0812', '0814', '0817', '0818', '0820', '0823', '0824', '0833', '0839', '0840', '0843', '0845', '0846', '0849', '0850', '0852', '0855', '0857', '0858', '0866', '0867', '0870', '0873', '0874', '0876', '0877', '0879', '0880', '0882', '0888', '0889', '0896', '0898', '0899', '0900', '0905', '0907', '0909', '0910', '0911', '0912', '0913', '0915', '0918', '0919', '0922', '0924', '0926', '0927', '0928', '0929', '0930', '0931', '0932', '0939', '0940', '0945', '0946', '0948', '0950', '0951', '0953', '0954', '0955', '0956', '0958', '0959', '0960', '0962', '0963', '0966', '0967', '0969', '0970', '0972', '0973', '0975', '0976', '0978', '0979', '0980', '0981', '0983', '0984', '0985', '0986', '0990', '0991', '0992', '0994', '0995', '0997', '0999', '1000', '1001', '1003', '1004', '1008', '1009', '1010', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1030', '1031', '1032', '1034', '1035', '1037', '1038', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1060', '1064', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1089', '1090', '1091', '1092', '1093', '1094', '1095', '1098', '1099', '1100', '1101', '1102', '1103', '1105', '1106', '1108', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '1120', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '1130', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1152', '1153', '1154', '1155', '1156', '1157', '1158', '1159', '1160', '1161', '1163', '1164', '1165', '1166', '1167', '1168', '1170', '1171', '1172', '1173', '1174', '1175', '1176', '1177', '1178', '1179', '1180', '1181', '1182', '1183', '1184', '1185', '1186', '1187', '1188', '1189', '1190', '1191', '1192', '1193', '1194', '1195', '1196', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1204', '1205', '1206', '1207', '1208', '1209', '1210', '1211', '1212', '1213', '1214', '1215', '1216', '1217', '1218', '1219', '1220', '1221', '1222', '1223', '1225', '1226', '1227', '1228', '1229', '1230', '1231', '1232', '1233', '1234', '1235', '1237', '1238', '1239', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1248', '1249', '1250', '1251', '1252', '1253', '1254', '1255', '1256', '1257', '1258', '1259', '1260', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1278', '1279', '1280', '1281', '1282', '1283', '1284', '1285', '1286', '1288', '1289', '1290', '1291', '1292', '1293', '1294', '1295', '1296', '1297', '1298', '1299', '1300', '1301', '1302', '1303', '1304', '1305', '1306', '1307', '1308', '1309', '1310', '1311', '1312', '1313', '1314', '1315', '1316', '1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '1325', '1326', '1327', '1328', '1329', '1330', '1332', '1333', '1334', '1335', '1336', '1337', '1338', '1339', '1340', '1341', '1342', '1343', '1345', '1346', '1347', '1348', '1349', '1350', '1351', '1352', '1353', '1354', '1355', '1356', '1357', '1358', '1359', '1360', '1361', '1362', '1363', '1364', '1365', '1367', '1368', '1369', '1370', '1371', '1372', '1373', '1374', '1375', '1377', '1378', '1379', '1380', '1381', '1382', '1383', '1384', '1385', '1386', '1387', '1389', '1390', '1391', '1392', '1393', '1394', '1395', '1396', '1397', '1398', '1399', '1400', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1409', '1410', '1411', '1412', '1413', '1414', '1415', '1416', '1417', '1418', '1419', '1420', '1421', '1422', '1423', '1424', '1425', '1426', '1427', '1428', '1429', '1430', '1431', '1432', '1433', '1434', '1435', '1436', '1437', '1438', '1439', '1440', '1441', '1442', '1443', '1444', '1445', '1446', '1447', '1448', '1449', '1450', '1451', '1452', '1453', '1455', '1456', '1457', '1458', '1459', '1460', '1461', '1462', '1463', '1464', '1465', '1466', '1467', '1468', '1469', '1470', '1471', '1472', '1473', '1474', '1475', '1476', '1477', '1478', '1479', '1480', '1481', '1482', '1483', '1484', '1485', '1486', '1488', '1489', '1490', '1491', '1492', '1493', '1494', '1495', '1496', '1497', '1498', '1499', '1500', '1501', '1502', '1503', '1504', '1505', '1506', '1508', '1509', '1510', '1511', '1512', '1513', '1514', '1515', '1516', '1517', '1518', '1519', '1520', '1521', '1522', '1523', '1524', '1525', '1526', '1527', '1528', '1529', '1530', '1531', '1532', '1533', '1534', '1535', '1536', '1537', '1538', '1539', '1540', '1541', '1542', '1543', '1544', '1545', '1546', '1547', '1548', '1549', '1550', '1551', '1552', '1553', '1554', '1555', '1556', '1557', '1558', '1559', '1560', '1561', '1562', '1563', '1564', '1565', '1566', '1567', '1568', '1569', '1570', '1571', '1572', '1573', '1574', '1575', '1576', '1577', '1578', '1579', '1580', '1581', '1582', '1583', '1584', '1585', '1586', '1587', '1588', '1589', '1590', '1591', '1592', '1593', '1594', '1595', '1596', '1597', '1598', '1599', '1600', '1601', '1602', '1603', '1604', '1605', '1606', '1607', '1608', '1609', '1610', '1611', '1612', '1613', '1614', '1615', '1616', '1617', '1618', '1619', '1620', '1621', '1622', '1623', '1624', '1625', '1626', '1627', '1628', '1629', '1630', '1631', '1632', '1633', '1634', '1635', '1636', '1637', '1638', '1639', '1640', '1641', '1642', '1643', '1644', '1645', '1646', '1647', '1648', '1649', '1650', '1651', '1652', '1653', '1654', '1655', '1656', '1657', '1658', '1659', '1660', '1661', '1662', '1663', '1664', '1665', '1666', '1667', '1668', '1669', '1670', '1671', '1672', '1673', '1674', '1675', '1676', '1677', '1678', '1679', '1680', '1681', '1682', '1683', '1684', '1685', '1686', '1687', '1688', '1689', '1690', '1691', '1692', '1693', '1694', '1695', '1696', '1697', '1698', '1699', '1700', '1701', '1702', '1703', '1704', '1705', '1706', '1707', '1708', '1709', '1710', '1711', '1712', '1713', '1714', '1715', '1716', '1717', '1718', '1719', '1720', '1721', '1722', '1723', '1724', '1725', '1726', '1727', '1728', '1729', '1730', '1731', '1732', '1733', '1734', '1735', '1736', '1737', '1738', '1739', '1740', '1741', '1742', '1743', '1744', '1745', '1746', '1747', '1748', '1749', '1750', '1751', '1752', '1753', '1754', '1755', '1756', '1757', '1758', '1759', '1760', '1761', '1762', '1763', '1764', '1765', '1766', '1767', '1768', '1769', '1770', '1771', '1772', '1773', '1774', '1775', '1776', '1777', '1778', '1779', '1780', '1781', '1782', '1783', '1784', '1785', '1786', '1787', '1788', '1789', '1790', '1791', '1792', '1793', '1794', '1795', '1796', '1797', '1798', '1799', '1800', '1801', '1802', '1803', '1804', '1805', '1806', '1807', '1808', '1809', '1810', '1811', '1812', '1813', '1814', '1815', '1816', '1817', '1818', '1819', '1820', '1821', '1822', '1823', '1824', '1825', '1826', '1827', '1828', '1829', '1830', '1831', '1832', '1833', '1834', '1835', '1836', '1837', '1838', '1839', '1840', '1841', '1842', '1843', '1844', '1845', '1846', '1847', '1848', '1849', '1850', '1851', '1852', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '1860', '1861', '1862', '1863', '1864', '1865', '1866', '1867', '1868', '1869', '1870', '1871', '1872', '1873', '1874', '1875', '1876', '1877', '1878', '1879', '1880', '1881', '1882', '1883', '1884', '1885', '1886', '1887', '1888', '1889', '1890', '1891', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '1900', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1940', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1950', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2161']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sorted_unique_values_third_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jgkg_WkRVIS",
        "outputId": "fbd79d47-3449-48d6-8e07-630f9f193ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1270"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_unique_integer(sorted_unique_values, interval_size=100, start_value=27530):\n",
        "    unique_integer_mapping = {}\n",
        "    current_interval_start = start_value\n",
        "\n",
        "    with open('/content/drive/MyDrive/output_mapping.txt', 'w') as output_file:\n",
        "        for value in sorted_unique_values:\n",
        "            current_value = int(value)\n",
        "            current_interval = current_value // interval_size\n",
        "\n",
        "            if current_interval not in unique_integer_mapping:\n",
        "                unique_integer_mapping[current_interval] = current_interval_start\n",
        "                current_interval_start += 1\n",
        "\n",
        "            output_line = f'<{value}>\\t{unique_integer_mapping[current_interval]}\\n'\n",
        "            output_file.write(output_line)\n",
        "\n",
        "    return unique_integer_mapping\n",
        "\n",
        "\n",
        "# Assign unique integers and get the mapping\n",
        "mapping = assign_unique_integer(sorted_unique_values_third_column)\n",
        "\n",
        "# Print the mapping\n",
        "print(\"Unique Integer Mapping:\")\n",
        "for interval, unique_integer in mapping.items():\n",
        "    print(f\"Interval {interval}: {unique_integer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzkZzPVYRVKT",
        "outputId": "6e64c5d3-e385-4204-b85b-777438524273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Integer Mapping:\n",
            "Interval 1: 27530\n",
            "Interval 2: 27531\n",
            "Interval 3: 27532\n",
            "Interval 4: 27533\n",
            "Interval 5: 27534\n",
            "Interval 6: 27535\n",
            "Interval 7: 27536\n",
            "Interval 8: 27537\n",
            "Interval 9: 27538\n",
            "Interval 10: 27539\n",
            "Interval 11: 27540\n",
            "Interval 12: 27541\n",
            "Interval 13: 27542\n",
            "Interval 14: 27543\n",
            "Interval 15: 27544\n",
            "Interval 16: 27545\n",
            "Interval 17: 27546\n",
            "Interval 18: 27547\n",
            "Interval 19: 27548\n",
            "Interval 20: 27549\n",
            "Interval 21: 27550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_files(file1_path, file2_path, output_file_path):\n",
        "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "        content1 = file1.read()\n",
        "        content2 = file2.read()\n",
        "\n",
        "    combined_content = content1 + content2\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        output_file.write(combined_content)\n",
        "\n",
        "# Example usage\n",
        "file1_path = '/content/drive/MyDrive/class_entity2id.txt'  # Replace with your actual file1 path\n",
        "file2_path = '/content/drive/MyDrive/output_mapping.txt'  # Replace with your actual file2 path\n",
        "output_file_path = '/content/drive/MyDrive/class_entity2id.txt'  # Replace with your desired output file path\n",
        "\n",
        "combine_files(file1_path, file2_path, output_file_path)"
      ],
      "metadata": {
        "id": "dwg7HNj6RVSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_symbols(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    # Remove < and > symbols from the first column\n",
        "    modified_lines = [line.split('\\t')[0].replace('<', '').replace('>', '').strip() + '\\t' + '\\t'.join(line.split('\\t')[1:]) for line in lines]\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        output_file.writelines(modified_lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/class_entity2id.txt'\n",
        "output_file_path = '/content/drive/MyDrive/USE_class_entity2id.txt'\n",
        "\n",
        "remove_symbols(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "ROPB-CaLsFDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/combined_data.txt'\n",
        "\n",
        "# Specify the number of lines to read\n",
        "num_lines_to_read = 15\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the specified number of lines\n",
        "    for _ in range(num_lines_to_read):\n",
        "        line = file.readline()\n",
        "\n",
        "        # Check if the line is not empty (end of file)\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "        # Process or print the line\n",
        "        print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cRjqMOWpfRw",
        "outputId": "bd32f8ca-22ec-45bc-f703-6d99c3942eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beverly_Adams\twasBornIn\tEdmonton\t1945-11-07\t1945-11-07\n",
            "Al_Gore\twasBornIn\tWashington,_D.C.\t1948-03-31\t1948-03-31\n",
            "Donald_R._McMonagle\twasBornIn\tFlint,_Michigan\t1952-05-14\t1952-05-14\n",
            "Donald_B._Beary\twasBornIn\tHelena,_Montana\t1888-12-04\t1888-12-04\n",
            "Ida_Halpern\twasBornIn\tVienna\t1910-07-17\t1910-07-17\n",
            "João_Paiva\twasBornIn\tLisbon\t1983-02-08\t1983-02-08\n",
            "Vincent_Wong_(Hong_Kong_actor)\twasBornIn\tHong_Kong\t1983-07-07\t1983-07-07\n",
            "Julie_Bishop_(actress)\twasBornIn\tDenver\t1914-08-30\t1914-08-30\n",
            "Bradley_Cooper\twasBornIn\tPhiladelphia\t1975-01-05\t1975-01-05\n",
            "José_Gonçalves_(footballer)\twasBornIn\tLisbon\t1985-09-17\t1985-09-17\n",
            "Harry_Weese\twasBornIn\tEvanston,_Illinois\t1915-06-30\t1915-06-30\n",
            "Roger_MacBride\twasBornIn\tNew_Rochelle,_New_York\t1929-08-06\t1929-08-06\n",
            "James_Neill\twasBornIn\tSavannah,_Georgia\t1860-09-29\t1860-09-29\n",
            "Wendy_Moniz\twasBornIn\tKansas_City,_Missouri\t1969-01-19\t1969-01-19\n",
            "Rob_Wagner\twasBornIn\tDetroit\t1872-08-02\t1872-08-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file path\n",
        "input_file_path = '/content/drive/MyDrive/combined_data.txt'\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/drive/MyDrive/class_relation2id.txt'\n",
        "\n",
        "# Dictionary to store unique values and corresponding integers\n",
        "unique_values_dict = {}\n",
        "\n",
        "# Read the input file and extract unique values\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    for line in input_file:\n",
        "        # Split the line into elements\n",
        "        elements = line.strip().split()\n",
        "\n",
        "        # Check if there's at least one element\n",
        "        if elements:\n",
        "            # Extract the first element\n",
        "            second_element = elements[1]\n",
        "\n",
        "            # If the value is not in the dictionary, add it with a unique integer\n",
        "            if second_element not in unique_values_dict:\n",
        "                unique_values_dict[second_element] = len(unique_values_dict)\n",
        "\n",
        "# Write unique values with <> symbols and unique integers to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for value, unique_integer in unique_values_dict.items():\n",
        "        output_file.write(f'<{value}>  \\t{unique_integer}\\n')"
      ],
      "metadata": {
        "id": "ZaZHzbOHpfUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file path\n",
        "input_file_path = '/content/drive/MyDrive/combined_data.txt'\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/content/drive/MyDrive/USE_class_relation2id.txt'\n",
        "\n",
        "# Dictionary to store unique values and corresponding integers\n",
        "unique_values_dict = {}\n",
        "\n",
        "# Read the input file and extract unique values\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    for line in input_file:\n",
        "        # Split the line into elements\n",
        "        elements = line.strip().split()\n",
        "\n",
        "        # Check if there's at least one element\n",
        "        if elements:\n",
        "            # Extract the first element\n",
        "            second_element = elements[1]\n",
        "\n",
        "            # If the value is not in the dictionary, add it with a unique integer\n",
        "            if second_element not in unique_values_dict:\n",
        "                unique_values_dict[second_element] = len(unique_values_dict)\n",
        "\n",
        "# Write unique values with <> symbols and unique integers to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for value, unique_integer in unique_values_dict.items():\n",
        "        output_file.write(f'{value}  \\t{unique_integer}\\n')"
      ],
      "metadata": {
        "id": "KipCo0EMDyed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path\n",
        "file_path = '/content/drive/MyDrive/class_relation2id.txt'\n",
        "\n",
        "# Specify the number of lines to read\n",
        "num_lines_to_read = 15\n",
        "\n",
        "# Open the file in read mode ('r')\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the specified number of lines\n",
        "    for _ in range(num_lines_to_read):\n",
        "        line = file.readline()\n",
        "\n",
        "        # Check if the line is not empty (end of file)\n",
        "        if not line:\n",
        "            break\n",
        "\n",
        "        # Process or print the line\n",
        "        print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_tWPKG9pfXh",
        "outputId": "fc3b28f1-3980-4f23-873d-0af106f690ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<wasBornIn>  \t0\n",
            "<worksAt>  \t1\n",
            "<playsFor>  \t2\n",
            "<hasWonPrize>  \t3\n",
            "<isMarriedTo>  \t4\n",
            "<owns>  \t5\n",
            "<graduatedFrom>  \t6\n",
            "<diedIn>  \t7\n",
            "<isAffiliatedTo>  \t8\n",
            "<created>  \t9\n",
            "<wasBornOnDate>  \t10\n",
            "<diedOnDate>  \t11\n",
            "<wasCreatedOnDate>  \t12\n",
            "<wasDestroyedOnDate>  \t13\n",
            "<happenedOnDate>  \t14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity2id_path = '/content/drive/MyDrive/USE_class_entity2id.txt'\n",
        "relation2id_path = '/content/drive/MyDrive/USE_class_relation2id.txt'\n",
        "\n",
        "# Read entity2id and relation2id mappings\n",
        "entity_mapping = {}\n",
        "relation_mapping = {}\n",
        "\n",
        "with open(entity2id_path, 'r') as entity_file:\n",
        "    for line in entity_file:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            entity_mapping[parts[0].strip()] = parts[1]\n",
        "\n",
        "with open(relation2id_path, 'r') as relation_file:\n",
        "    for line in relation_file:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            relation_mapping[parts[0].strip()] = parts[1]"
      ],
      "metadata": {
        "id": "IaQYW88XrAXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_values_to_names(input_file_path, output_file_path, entity_mapping, relation_mapping):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for line in lines:\n",
        "            elements = line.strip().split('\\t')\n",
        "            if len(elements) >= 3:\n",
        "                # Replace entity values in the 1st and 3rd columns\n",
        "                entity1 = entity_mapping.get(elements[0], elements[0])\n",
        "                entity2 = entity_mapping.get(elements[2], elements[2])\n",
        "\n",
        "                # Replace relation value in the 2nd column\n",
        "                relation = relation_mapping.get(elements[1], elements[1])\n",
        "\n",
        "                # Keep the 4th and 5th columns unchanged\n",
        "                output_line = f\"{entity1}\\t{relation}\\t{entity2}\\t{elements[3]}\\t{elements[4]}\\n\"\n",
        "\n",
        "                output_file.write(output_line)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/combined_data.txt'  # Replace with your actual input file path\n",
        "output_file_path = '/content/drive/MyDrive/class_triple2id.txt'  # Replace with your desired output file path\n",
        "\n",
        "\n",
        "map_values_to_names(input_file_path, output_file_path, entity_mapping, relation_mapping)"
      ],
      "metadata": {
        "id": "Ivw1BzDqrAas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_values_to_names(input_file_path, output_file_path, entity_mapping, relation_mapping):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for line in lines:\n",
        "            elements = line.strip().split('\\t')\n",
        "            if len(elements) >= 3:\n",
        "                # Replace entity values in the 1st and 3rd columns\n",
        "                entity1 = entity_mapping.get(elements[0], elements[0])\n",
        "                entity2 = entity_mapping.get(elements[2], elements[2])\n",
        "\n",
        "                # Replace relation value in the 2nd column\n",
        "                relation = relation_mapping.get(elements[1], elements[1])\n",
        "\n",
        "                # Keep the 4th and 5th columns unchanged\n",
        "                output_line = f\"{entity1}\\t{relation}\\t{entity2}\\t{elements[3]}\\t{elements[4]}\\n\"\n",
        "\n",
        "                output_file.write(output_line)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/combined_data.txt'  # Replace with your actual input file path\n",
        "output_file_path = '/content/drive/MyDrive/COPY_class_triple2id.txt'  # Replace with your desired output file path\n",
        "\n",
        "\n",
        "map_values_to_names(input_file_path, output_file_path, entity_mapping, relation_mapping)"
      ],
      "metadata": {
        "id": "n97kRwlcrAdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def shuffle_lines(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "        random.shuffle(lines)\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        output_file.writelines(lines)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/COPY_class_triple2id.txt'  # Replace with your actual input file path\n",
        "output_file_path = '/content/drive/MyDrive/COPY_class_triple2id.txt' # Replace with your desired output file path\n",
        "\n",
        "shuffle_lines(input_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "Z9jO_PNUpfaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_set_and_remove(input_file_path, train_file_path):\n",
        "    # Read the content of the input file\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    # Extract unique values from the first column and the third column:\n",
        "    unique_values = set()\n",
        "    for line in lines:\n",
        "        elements = line.strip().split('\\t')\n",
        "        if len(elements) >= 4:\n",
        "            unique_values.add(elements[0])\n",
        "            unique_values.add(elements[2])\n",
        "\n",
        "    # Initialize the training set and set to track included values\n",
        "    training_set = []\n",
        "    included_values = set()\n",
        "\n",
        "    # Iterate through lines to create the training set\n",
        "    for line in lines:\n",
        "        value1 = line.split()[0]\n",
        "        value2 = line.split()[2]\n",
        "\n",
        "        # Include the line in the training set if it has a unique value\n",
        "        if value1 in unique_values and value1 not in included_values:\n",
        "            training_set.append(line)\n",
        "            included_values.add(value1)\n",
        "            included_values.add(value2)\n",
        "\n",
        "        if value2 in unique_values and value2 not in included_values:\n",
        "            training_set.append(line)\n",
        "            included_values.add(value1)\n",
        "            included_values.add(value2)\n",
        "\n",
        "\n",
        "    # Write the training set to the training file\n",
        "    with open(train_file_path, 'w') as train_file:\n",
        "        train_file.writelines(training_set)\n",
        "\n",
        "    # Remove included data points from the input file\n",
        "    lines = [line for line in lines if line not in training_set]\n",
        "    with open(input_file_path, 'w') as input_file:\n",
        "        input_file.writelines(lines)\n",
        "\n",
        "# Define file paths\n",
        "# Define file paths\n",
        "input_file_path = '/content/drive/MyDrive/COPY_class_triple2id.txt'\n",
        "train_file_path = '/content/drive/MyDrive/train_data.txt'\n",
        "\n",
        "# Create the training set and remove data points from the input file\n",
        "create_training_set_and_remove(input_file_path, train_file_path)"
      ],
      "metadata": {
        "id": "Vc5LTkDTpfdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_lines_with_condition(input_file_path):\n",
        "    # Read the content of the input file\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    # Identify line numbers where the 3rd element has length != 4\n",
        "    lines_with_condition = []\n",
        "    for i, line in enumerate(lines):\n",
        "        elements = line.strip().split('\\t')\n",
        "        if len(elements[2]) > 5:\n",
        "            lines_with_condition.append(i + 1)  # Adding 1 to convert from zero-based index to line number\n",
        "\n",
        "    return lines_with_condition\n",
        "\n",
        "# Define file path\n",
        "input_file_path = '/content/drive/MyDrive/class_triple2id.txt'\n",
        "\n",
        "# Find line numbers where the 3rd element has length != 5\n",
        "lines_with_condition = find_lines_with_condition(input_file_path)"
      ],
      "metadata": {
        "id": "pT32HQJkpfg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_with_condition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfpBtvEZpfj0",
        "outputId": "5f4e7087-c6d9-484b-ca33-d96ffb4d7711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcSnb-Z5pfm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}